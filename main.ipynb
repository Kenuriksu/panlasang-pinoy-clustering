{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient-Based Clustering of Pinoy Dishes\n",
    "\n",
    "The ambition is to identify clusters of Filipino dishes based on their ingredients. The results can be used to define the most common ingredients of a Filipino pantry. This project leverages techniques from data mining, natural language processing, and unsupervised learning. The dataset is a collection of Filipino recipes from various online recipe websites. The dataset contains the name, ingredients, and instructions of the recipes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "RECIPES_PATH = \"data/recipes\"\n",
    "\n",
    "\n",
    "def load_recipes(path):\n",
    "    \"\"\"Combines all recipe data into a single list.\"\"\"\n",
    "    files = [file for file in os.listdir(path)]\n",
    "\n",
    "    recipes = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            recipe_data = json.load(f)\n",
    "            recipes.extend(recipe_data)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "\n",
    "recipes = load_recipes(RECIPES_PATH)\n",
    "recipes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "recipe_df = pd.DataFrame(recipes)\n",
    "recipe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_ingredient_analysis(\n",
    "    ingredient_series,\n",
    "    n=30,\n",
    "    most_common=True,\n",
    "    figsize=(8, 10),\n",
    "):\n",
    "    \"\"\"Plots ingredient distributions for most/least common ingredients.\"\"\"\n",
    "\n",
    "    all_ingredients = ingredient_series.dropna().explode()\n",
    "    ingredient_counts = all_ingredients.value_counts()\n",
    "\n",
    "    _, axs = plt.subplots(\n",
    "        2,\n",
    "        1,\n",
    "        figsize=figsize,\n",
    "        gridspec_kw={\"height_ratios\": [1, 5]},\n",
    "    )\n",
    "\n",
    "    counts = ingredient_series.dropna().apply(len)\n",
    "    sns.histplot(counts, kde=True, binwidth=1, ax=axs[0])\n",
    "    axs[0].set_title(f\"Number of ingredients in {len(counts)} recipes\")\n",
    "    axs[0].set_xlabel(\"Number of ingredients\")\n",
    "    axs[0].set_ylabel(\"Number of recipes\")\n",
    "\n",
    "    if most_common:\n",
    "        top_n_ingredients = ingredient_counts.head(n)\n",
    "    else:\n",
    "        top_n_ingredients = ingredient_counts.tail(n)\n",
    "\n",
    "    ylabels = [\n",
    "        f\"{i[:20]:>20}{'...' if len(i) > 20 else ''}\"\n",
    "        for i in top_n_ingredients.index\n",
    "    ]\n",
    "    sns.barplot(x=top_n_ingredients.values, y=ylabels, ax=axs[1])\n",
    "\n",
    "    indicator = \"Most\" if most_common else \"Least\"\n",
    "    axs[1].set_title(f\"Top {n} {indicator} Common Ingredients\")\n",
    "    axs[1].set_xlabel(\"Number of Recipes\")\n",
    "    axs[1].set_ylabel(\"Ingredient\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_ingredient_analysis(recipe_df[\"ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove recipes with no ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df = recipe_df.dropna(subset=[\"ingredients\"])\n",
    "recipe_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text for manageable pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tabulate import tabulate\n",
    "\n",
    "SAMPLE_INGREDIENT = \"small lemon lemons or 6 to 7 pieces calamansi\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def display_tokens(text):\n",
    "    \"\"\"Display token attributes of a given string.\"\"\",\n",
    "    rows = []\n",
    "    for token in nlp(text):\n",
    "        row = [\n",
    "            token.text,\n",
    "            token.lemma_,\n",
    "            token.pos_,\n",
    "            token.tag_,\n",
    "            token.dep_,\n",
    "            token.is_alpha,\n",
    "        ]\n",
    "        rows.append(row)\n",
    "    headers = [\"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"is_alpha\"]\n",
    "    print(tabulate(rows, headers=headers))\n",
    "\n",
    "\n",
    "display_tokens(SAMPLE_INGREDIENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize words and remove non-ingredient words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_pos_tags(token):\n",
    "    \"\"\"Fixes POS tags for certain words in a string.\"\"\",\n",
    "    if token.text in [\"cauliflower\", \"baking\"]:\n",
    "        token.pos_ = \"NOUN\"  # Override POS tag for some words\n",
    "    return token\n",
    "\n",
    "\n",
    "def lemmatize_valid_nouns(ingredient):\n",
    "    \"\"\"Lemmatize valid nouns in a string.\"\"\",\n",
    "    doc = nlp(ingredient)\n",
    "    valid_lemmas = []\n",
    "    for token in doc:\n",
    "        token = fix_pos_tags(token)\n",
    "        if (\n",
    "            token.is_alpha\n",
    "            and not token.is_stop\n",
    "            and token.pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        ):\n",
    "            valid_lemmas.append(token.lemma_)\n",
    "    return \" \".join(valid_lemmas)\n",
    "\n",
    "\n",
    "lemmatized = lemmatize_valid_nouns(SAMPLE_INGREDIENT)\n",
    "display_tokens(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words related to culinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/culinary_stopwords.txt\", encoding=\"utf-8\") as file:\n",
    "    stopwords = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    \"\"\"Filter out stopwords from a string.\"\"\"\n",
    "    meaningful_terms = []\n",
    "    for term in text.split():\n",
    "        if term not in stopwords:\n",
    "            meaningful_terms.append(term)\n",
    "    return \" \".join(meaningful_terms)\n",
    "\n",
    "\n",
    "filtered = filter_stopwords(lemmatized)\n",
    "display_tokens(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle synonyms at word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/word_synonyms.json\") as file:\n",
    "    word_synonyms = json.load(file)\n",
    "\n",
    "\n",
    "def handle_word_synonym(ingredient):\n",
    "    \"\"\"Replace ingredient synonyms with a common name.\"\"\"\n",
    "    common_words = []\n",
    "    for word in ingredient.split():\n",
    "        common_words.append(word_synonyms.get(word, word))\n",
    "    return \" \".join(common_words)\n",
    "\n",
    "\n",
    "normalized_words = handle_word_synonym(filtered)\n",
    "display_tokens(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    \"\"\"Remove duplicate terms while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for ingredient in text.split():\n",
    "        if ingredient not in seen:\n",
    "            unique.append(ingredient)\n",
    "            seen.add(ingredient)\n",
    "    return \" \".join(unique)\n",
    "\n",
    "unique = remove_duplicates(normalized_words)\n",
    "display_tokens(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle synonyms at phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/phrase_synonyms.json\") as file:\n",
    "    phrase_synonyms = json.load(file)\n",
    "\n",
    "\n",
    "def handle_phrase_synonym(ingredient):\n",
    "    \"\"\"Replace ingredient synonyms with a common name.\"\"\"\n",
    "    return phrase_synonyms.get(ingredient, ingredient)\n",
    "\n",
    "common_name = handle_phrase_synonym(unique)\n",
    "display_tokens(common_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add flavor tags to ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ingredient_to_flavor(ingredient):\n",
    "    \"\"\"Maps an ingredient to a flavor.\"\"\"\n",
    "    with open(\"data/flavor_map.json\") as file:\n",
    "        flavor_map_data = json.load(file)\n",
    "\n",
    "    ingredient_flavors = {}\n",
    "    for entry in flavor_map_data:\n",
    "        for flavor, ingredients in entry.items():\n",
    "            for i in ingredients:\n",
    "                ingredient_flavors[i] = flavor\n",
    "\n",
    "    flavor = ingredient_flavors.get(ingredient, \"\")\n",
    "    return f\"{flavor} {ingredient}\"\n",
    "\n",
    "\n",
    "map_ingredient_to_flavor(common_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def filter_common_ingredient(ingredient):\n",
    "    commons = [\n",
    "        \"water\",\n",
    "        \"oil\",\n",
    "        \"salt\",\n",
    "        \"onion\",\n",
    "        \"sugar\",\n",
    "        \"pepper\",\n",
    "        \"garlic\",\n",
    "    ]\n",
    "    return ingredient if ingredient not in commons else None\n",
    "\n",
    "\n",
    "def preprocess_ingredient(ingredient):\n",
    "    \"\"\"Preprocess an ingredient string to extract key terms.\"\"\",\n",
    "    no_parenthesis = re.sub(r\"\\([^)]*\\)\", \"\", ingredient)\n",
    "    first_option = no_parenthesis.split(\" or \")[0]\n",
    "    formatted = first_option.lower().strip()\n",
    "\n",
    "    lemmatized = lemmatize_valid_nouns(formatted)\n",
    "    filtered = filter_stopwords(lemmatized)\n",
    "    word_synonym = handle_word_synonym(filtered)\n",
    "    unique = remove_duplicates(word_synonym)\n",
    "    phrase_synonym = handle_phrase_synonym(unique)\n",
    "\n",
    "    if isinstance(phrase_synonym, str):\n",
    "        valid_ingredient = filter_common_ingredient(phrase_synonym)\n",
    "        with_flavor = map_ingredient_to_flavor(valid_ingredient)\n",
    "    elif isinstance(phrase_synonym, list):\n",
    "        valid_ingredient = [\n",
    "            filter_common_ingredient(phrase) for phrase in phrase_synonym\n",
    "        ]\n",
    "        with_flavor = [\n",
    "            map_ingredient_to_flavor(phrase) for phrase in valid_ingredient\n",
    "        ]\n",
    "\n",
    "    return with_flavor\n",
    "\n",
    "\n",
    "clean_ingredient = preprocess_ingredient(SAMPLE_INGREDIENT)\n",
    "clean_ingredient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def clean_ingredients(ingredients):\n",
    "    \"\"\"Applies preprocessing to a list of ingredients and flattens the list.\"\"\"\n",
    "    cleaned_ingredients = []\n",
    "    for ingredient in ingredients:\n",
    "        cleaned = preprocess_ingredient(ingredient)\n",
    "        if isinstance(cleaned, list):\n",
    "            cleaned_ingredients.extend(cleaned)\n",
    "        else:\n",
    "            cleaned_ingredients.append(cleaned)\n",
    "\n",
    "    return list(sorted(set(filter(None, cleaned_ingredients))))\n",
    "\n",
    "\n",
    "recipe_df[\"cleaned_ingredients\"] = (\n",
    "    recipe_df[\"ingredients\"].progress_apply(clean_ingredients)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ingredient_analysis(recipe_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save list of preprocessed ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = recipe_df[\"cleaned_ingredients\"]\n",
    "cleaned = cleaned.dropna().explode().unique().astype(str)\n",
    "cleaned.sort()\n",
    "\n",
    "filename = f\"data/cleaned_ingredients.txt\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(\"\\n\".join(cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQUENCY = 5\n",
    "\n",
    "counts = recipe_df[\"cleaned_ingredients\"].explode().value_counts()\n",
    "rare_ingredients = counts[counts <= MIN_FREQUENCY].index\n",
    "\n",
    "with open(\"data/rare_ingredients.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(\"\\n\".join(sorted(rare_ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_ingredients(ingredients):\n",
    "    \"\"\"Remove rare ingredients from a list.\"\"\"\n",
    "    return [\n",
    "        ingredient\n",
    "        for ingredient in ingredients\n",
    "        if ingredient not in rare_ingredients\n",
    "    ]\n",
    "\n",
    "reduced_df = recipe_df.copy()\n",
    "\n",
    "reduced_df.loc[:, \"cleaned_ingredients\"] = (\n",
    "    reduced_df[\"cleaned_ingredients\"].apply(remove_common_ingredients)\n",
    ")\n",
    "\n",
    "plot_ingredient_analysis(reduced_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ingredient_analysis(reduced_df[\"cleaned_ingredients\"], most_common=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove recipes with too few or too many ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the 3rd quartile minus 1st quartile (75%)\n",
    "\n",
    "MIN_INGREDIENT = 4\n",
    "MAX_INGREDIENT = 9\n",
    "\n",
    "counts = reduced_df[\"cleaned_ingredients\"].apply(len)\n",
    "reduced_df = reduced_df[(counts >= MIN_INGREDIENT) & (counts <= MAX_INGREDIENT)]\n",
    "\n",
    "plot_ingredient_analysis(reduced_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "ingredients_as_texts = [\n",
    "    \" \".join(sorted(ingredients))\n",
    "    for ingredients in reduced_df[\"cleaned_ingredients\"]\n",
    "]\n",
    "embeddings = model.encode(\n",
    "    ingredients_as_texts,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "dimensions_to_keep = 2\n",
    "svd = TruncatedSVD(n_components=dimensions_to_keep)\n",
    "reduced_embeddings = svd.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 7\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "cluster_labels = clustering.fit_predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_clusters_3d(reduced_embeddings, cluster_labels, recipe_df):\n",
    "    \"\"\"Plots a 3D scatter plot of recipe clusters.\"\"\"\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": reduced_embeddings[:, 0],\n",
    "            \"y\": reduced_embeddings[:, 1],\n",
    "            # \"z\": reduced_embeddings[:, 2],\n",
    "            \"cluster\": cluster_labels.astype(str),\n",
    "            \"recipe_name\": recipe_df[\"name\"],\n",
    "            \"cleaned_ingredients\": [\n",
    "                \"<br>\".join(ingredients)\n",
    "                for ingredients in recipe_df[\"cleaned_ingredients\"]\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig = px.scatter(\n",
    "        final_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        # z=\"z\",\n",
    "        color=\"cluster\",\n",
    "        hover_name=\"recipe_name\",\n",
    "        hover_data=[\"cleaned_ingredients\"],\n",
    "        width=800,\n",
    "        height=1000,\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.update_layout(title=\"Ingredient-Based Clustering of Filipino Dishes\")\n",
    "\n",
    "    unique_clusters = final_df[\"cluster\"].unique()\n",
    "    buttons = [\n",
    "        dict(\n",
    "            label=f\"Cluster {cluster}\",\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"visible\": [cluster == c for c in unique_clusters]},\n",
    "                {\"title\": f\"Ingredient-Based Clustering - Cluster {cluster}\"},\n",
    "            ],\n",
    "        )\n",
    "        for cluster in unique_clusters\n",
    "    ]\n",
    "    fig.update_layout(\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type=\"buttons\",\n",
    "                direction=\"down\",\n",
    "                buttons=buttons,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_clusters_3d(reduced_embeddings, cluster_labels, reduced_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
