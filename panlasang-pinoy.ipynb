{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient-Based Clustering of Pinoy Dishes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"panlasang_pinoy_recipes.json\") as file:\n",
    "    recipes = json.load(file)\n",
    "\n",
    "recipes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recipes)  # 1873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_unique_ingredients(recipes):\n",
    "    ingredients = set()\n",
    "    for recipe in recipes:\n",
    "        for ingredient in recipe[\"ingredients\"]:\n",
    "            ingredients.add(ingredient)\n",
    "    return list(ingredients)\n",
    "\n",
    "\n",
    "all_ingredients = get_all_unique_ingredients(recipes)\n",
    "len(all_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_ingredients.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(\"\\n\".join(sorted(all_ingredients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_str_as_doc(text):\n",
    "    \"\"\"Display a spaCy doc in a table format.\"\"\"\n",
    "    rows = []\n",
    "    for token in nlp(text):\n",
    "        row = [\n",
    "            token.text,\n",
    "            token.lemma_,\n",
    "            token.pos_,\n",
    "            token.tag_,\n",
    "            token.dep_,\n",
    "            token.is_alpha,\n",
    "        ]\n",
    "        rows.append(row)\n",
    "    headers = [\"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"is_alpha\"]\n",
    "    print(tabulate(rows, headers=headers))\n",
    "\n",
    "\n",
    "SAMPLE_INGREDIENT = \"14 oz. bean curd, sliced into 1/2 inch thick flat pieces\"\n",
    "display_str_as_doc(SAMPLE_INGREDIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_valid_nouns(ingredient):\n",
    "    \"\"\"Lemmatize valid nouns in a spaCy doc.\"\"\"\n",
    "    doc = nlp(ingredient)\n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            clean_tokens.append(token.lemma_)\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "\n",
    "lemmatized = lemmatize_valid_nouns(SAMPLE_INGREDIENT)\n",
    "display_str_as_doc(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"culinary_stopwords.txt\") as file:\n",
    "    culinary_stopwords = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "def filter_stopwords(terms):\n",
    "    \"\"\"Filter out culinary stopwords from a string of terms.\"\"\"\n",
    "    clean_terms = []\n",
    "    for term in terms.split():\n",
    "        if term not in culinary_stopwords:\n",
    "            clean_terms.append(term)\n",
    "    return \" \".join(clean_terms)\n",
    "\n",
    "\n",
    "filtered = filter_stopwords(lemmatized)\n",
    "display_str_as_doc(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ingredient_synonyms.json\") as file:\n",
    "    ingredient_synonyms = json.load(file)\n",
    "\n",
    "\n",
    "def handle_synonyms(ingredient):\n",
    "    \"\"\"Replace ingredient synonyms with a common name.\"\"\"\n",
    "    return ingredient_synonyms.get(ingredient, ingredient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ingredient(ingredient):\n",
    "    \"\"\"Preprocess an ingredient string to extract key terms.\"\"\"\n",
    "    # Remove any text in parentheses\n",
    "    ingredient = ingredient.split(\"(\")[0]\n",
    "\n",
    "    # Get the first item in a list of alternatives\n",
    "    if \" or \" in ingredient:\n",
    "        ingredient = ingredient.split(\" or \")[0]\n",
    "\n",
    "    lowered = ingredient.lower().strip()\n",
    "    lemmatized = lemmatize_valid_nouns(lowered)\n",
    "    filtered = filter_stopwords(lemmatized)\n",
    "    common_name = handle_synonyms(filtered)\n",
    "    return common_name\n",
    "\n",
    "\n",
    "clean_ingredient = preprocess_ingredient(SAMPLE_INGREDIENT)\n",
    "clean_ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_by_recipe = []\n",
    "\n",
    "for recipe in tqdm(recipes):\n",
    "    ingredients = set()\n",
    "    for terms in recipe[\"ingredients\"]:\n",
    "        clean_terms = preprocess_ingredient(terms)\n",
    "        if clean_terms:\n",
    "            ingredients.add(clean_terms)\n",
    "    ingredients_by_recipe.append(\" \".join(sorted(ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_by_recipe[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cleaned_ingredients = set()\n",
    "for recipe in ingredients_by_recipe:\n",
    "    for ingredient in recipe:\n",
    "        all_cleaned_ingredients.add(ingredient)\n",
    "\n",
    "ingredient_pairs = dict(sorted((i, i) for i in all_cleaned_ingredients))\n",
    "\n",
    "with open(f\"ingredient_synonyms.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(ingredient_pairs, file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "embeddings = vectorizer.fit_transform(ingredients_by_recipe)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(\n",
    "    [\" \".join(ingredients) for ingredients in ingredients_by_recipe],\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "dimensions_to_keep = 3  # For 3D visualization\n",
    "svd = TruncatedSVD(n_components=dimensions_to_keep)\n",
    "reduced_embeddings = svd.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 7\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "cluster_labels = clustering.fit_predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": reduced_embeddings[:, 0],\n",
    "        \"y\": reduced_embeddings[:, 1],\n",
    "        \"z\": reduced_embeddings[:, 2],\n",
    "        \"cluster\": cluster_labels,\n",
    "        \"recipe_name\": [recipe[\"name\"] for recipe in recipes],\n",
    "        \"ingredients\": ingredients_by_recipe,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    z=\"z\",\n",
    "    color=\"cluster\",\n",
    "    hover_name=\"recipe_name\",\n",
    "    hover_data=[\"ingredients\"],\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title=\"Ingredient-Based Clustering of Filipino Dishes\")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
