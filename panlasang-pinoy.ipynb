{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient-Based Clustering of Pinoy Dishes\n",
    "\n",
    "The ambition is to identify clusters of Filipino dishes based on their ingredients. The results can be used to define the most common ingredients of a Filipino pantry. This project leverages techniques from data mining, natural language processing, and unsupervised learning. The dataset is a collection of Filipino recipes from various online recipe websites. The dataset contains the name, ingredients, and instructions of the recipes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_recipe_data(path=\"data/recipes\"):\n",
    "    \"\"\"Combines all recipe data into a single list.\"\"\"\n",
    "    files = [file for file in os.listdir(path)]\n",
    "\n",
    "    recipes = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(path, file)\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            recipe_data = json.load(f)\n",
    "            recipes.extend(recipe_data)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "\n",
    "recipes = merge_recipe_data()\n",
    "recipe_df = pd.DataFrame(recipes)\n",
    "recipe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover patterns through visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_ingredient_analysis(\n",
    "    ingredient_series,\n",
    "    n=30,\n",
    "    show_distribution=True,\n",
    "    most_common=True,\n",
    "    figsize=(8, 10),\n",
    "):\n",
    "    \"\"\"Plots ingredient distributions and displays most/least common ingredients.\n",
    "\n",
    "    Args:\n",
    "        ingredient_series: A Series containing lists of ingredients.\n",
    "        n: Number of top/bottom ingredients to display.\n",
    "        show_distribution: If True, shows the ingredient distribution plot.\n",
    "        most_common: If True, shows the most common ingredients.\n",
    "        figsize: Size of the plot figure.\n",
    "    \"\"\"\n",
    "\n",
    "    all_ingredients = ingredient_series.dropna().explode()\n",
    "    ingredient_counts = all_ingredients.value_counts()\n",
    "\n",
    "    _, axs = plt.subplots(2, 1, figsize=figsize, gridspec_kw={\"height_ratios\": [1, 5]})\n",
    "\n",
    "    if show_distribution:\n",
    "        counts = ingredient_series.dropna().apply(len)\n",
    "        sns.histplot(counts, kde=True, binwidth=1, ax=axs[0])\n",
    "        axs[0].set_title(f\"Number of ingredients in {len(counts)} recipes\")\n",
    "        axs[0].set_xlabel(\"Number of ingredients\")\n",
    "        axs[0].set_ylabel(\"Number of recipes\")\n",
    "\n",
    "    if most_common:\n",
    "        top_n_ingredients = ingredient_counts.head(n)\n",
    "    else:\n",
    "        top_n_ingredients = ingredient_counts.tail(n)\n",
    "\n",
    "    ylabels = [\n",
    "        f\"{i[:20]:>20}{\"...\" if len(i) > 20 else \"\"}\"\n",
    "        for i in top_n_ingredients.index\n",
    "    ]\n",
    "    sns.barplot(\n",
    "        x=top_n_ingredients.values,\n",
    "        y=ylabels,\n",
    "        ax=axs[1]\n",
    "    )\n",
    "\n",
    "    indicator = \"Most\" if most_common else \"Least\"\n",
    "    axs[1].set_title(f\"Top {n} {indicator} Common Ingredients\")\n",
    "    axs[1].set_xlabel(\"Number of Recipes\")\n",
    "    axs[1].set_ylabel(\"Ingredient\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ingredient_analysis(recipe_df[\"ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove recipes with no ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df = recipe_df.dropna(subset=[\"ingredients\"])\n",
    "recipe_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tabulate import tabulate\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def display_tokens(text):\n",
    "    \"\"\"Display token attributes of a given string.\"\"\",\n",
    "    rows = []\n",
    "    for token in nlp(text):\n",
    "        row = [\n",
    "            token.text,\n",
    "            token.lemma_,\n",
    "            token.pos_,\n",
    "            token.tag_,\n",
    "            token.dep_,\n",
    "            token.is_alpha,\n",
    "        ]\n",
    "        rows.append(row)\n",
    "    headers = [\"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"is_alpha\"]\n",
    "    print(tabulate(rows, headers=headers))\n",
    "\n",
    "\n",
    "SAMPLE_INGREDIENT = \"small lemon lemons or 6 to 7 pieces calamansi\"\n",
    "display_tokens(SAMPLE_INGREDIENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize and remove non-ingredient words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_valid_nouns(ingredient):\n",
    "    \"\"\"Lemmatize valid nouns in a string.\"\"\",\n",
    "    doc = nlp(ingredient)\n",
    "    valid_lemmas = []\n",
    "    for token in doc:\n",
    "        outliers = [\"cauliflower\", \"baking\"]\n",
    "        if token.text in outliers:\n",
    "            token.pos_ = \"NOUN\"  # Override POS tag for some words\n",
    "\n",
    "        if (\n",
    "            token.is_alpha\n",
    "            and not token.is_stop\n",
    "            and token.pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        ):\n",
    "            valid_lemmas.append(token.lemma_)\n",
    "    return \" \".join(valid_lemmas)\n",
    "\n",
    "\n",
    "lemmatized = lemmatize_valid_nouns(SAMPLE_INGREDIENT)\n",
    "display_tokens(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove culinary stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/culinary_stopwords.txt\", encoding=\"utf-8\") as file:\n",
    "    stopwords = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    \"\"\"Filter out stopwords from a string.\"\"\"\n",
    "    meaningful_terms = []\n",
    "    for term in text.split():\n",
    "        if term not in stopwords:\n",
    "            meaningful_terms.append(term)\n",
    "    return \" \".join(meaningful_terms)\n",
    "\n",
    "\n",
    "filtered = filter_stopwords(lemmatized)\n",
    "display_tokens(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle synonyms at word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/word_synonyms.json\") as file:\n",
    "    word_synonyms = json.load(file)\n",
    "\n",
    "\n",
    "def handle_word_synonym(ingredient):\n",
    "    \"\"\"Replace ingredient synonyms with a common name.\"\"\"\n",
    "    common_words = []\n",
    "    for word in ingredient.split():\n",
    "        common_words.append(word_synonyms.get(word, word))\n",
    "    return \" \".join(common_words)\n",
    "\n",
    "\n",
    "normalized_words = handle_word_synonym(filtered)\n",
    "display_tokens(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    \"\"\"Remove duplicate terms while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for ingredient in text.split():\n",
    "        if ingredient not in seen:\n",
    "            unique.append(ingredient)\n",
    "            seen.add(ingredient)\n",
    "    return \" \".join(unique)\n",
    "\n",
    "unique = remove_duplicates(normalized_words)\n",
    "display_tokens(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle synonyms to standardize ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/phrase_synonyms.json\") as file:\n",
    "    phrase_synonyms = json.load(file)\n",
    "\n",
    "\n",
    "def handle_phrase_synonym(ingredient):\n",
    "    \"\"\"Replace ingredient synonyms with a common name.\"\"\"\n",
    "    return phrase_synonyms.get(ingredient, ingredient)\n",
    "\n",
    "common_name = handle_phrase_synonym(unique)\n",
    "display_tokens(common_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the entire preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess_ingredient(ingredient):\n",
    "    \"\"\"Preprocess an ingredient string to extract key terms.\"\"\",\n",
    "    no_parenthesis = re.sub(r\"\\([^)]*\\)\", \"\", ingredient)\n",
    "    first_option = no_parenthesis.split(\" or \")[0]\n",
    "    formatted = first_option.lower().strip()\n",
    "\n",
    "    lemmatized = lemmatize_valid_nouns(formatted)\n",
    "    filtered = filter_stopwords(lemmatized)\n",
    "    word_synonym = handle_word_synonym(filtered)\n",
    "    unique = remove_duplicates(word_synonym)\n",
    "    phrase_synonym = handle_phrase_synonym(unique)\n",
    "\n",
    "    return phrase_synonym\n",
    "\n",
    "\n",
    "clean_ingredient = preprocess_ingredient(SAMPLE_INGREDIENT)\n",
    "clean_ingredient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def clean_ingredients(ingredients):\n",
    "    \"\"\"Applies preprocessing to a list of ingredients and flattens the list.\"\"\"\n",
    "    cleaned_ingredients = []\n",
    "    for ingredient in ingredients:\n",
    "        cleaned = preprocess_ingredient(ingredient)\n",
    "        if isinstance(cleaned, list):\n",
    "            cleaned_ingredients.extend(cleaned)\n",
    "        else:\n",
    "            cleaned_ingredients.append(cleaned)\n",
    "\n",
    "    return list(sorted(set(filter(None, cleaned_ingredients))))\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "recipe_df[\"cleaned_ingredients\"] = (\n",
    "    recipe_df[\"ingredients\"].progress_apply(clean_ingredients)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ingredient_analysis(recipe_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ingredients(ingredient_series):\n",
    "    ingredients = ingredient_series.dropna().explode().unique().astype(str)\n",
    "    ingredients.sort()\n",
    "\n",
    "    filename = f\"data/{ingredient_series.name}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"\\n\".join(ingredients))\n",
    "\n",
    "\n",
    "save_ingredients(recipe_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df[[\"ingredients\", \"cleaned_ingredients\"]].to_json(\n",
    "    \"data/cleaned_recipes.json\",\n",
    "    indent=2,\n",
    "    orient=\"records\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove common ingredients not specific to Filipino cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_to_remove = [\n",
    "    \"oil\",\n",
    "    \"salt\",\n",
    "    \"onion\",\n",
    "    \"sugar\",\n",
    "    \"pepper\",\n",
    "    \"garlic\",\n",
    "]\n",
    "\n",
    "\n",
    "def remove_rare_ingredients(ingredients):\n",
    "    return [i for i in ingredients if i not in ingredients_to_remove]\n",
    "\n",
    "\n",
    "reduced_df = recipe_df.copy()\n",
    "\n",
    "reduced_df[\"cleaned_ingredients\"] = (\n",
    "    reduced_df[\"cleaned_ingredients\"].apply(remove_rare_ingredients)\n",
    ")\n",
    "\n",
    "plot_ingredient_analysis(reduced_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQUENCY = 5\n",
    "\n",
    "counts = recipe_df[\"cleaned_ingredients\"].explode().value_counts()\n",
    "rare_ingredients = counts[counts <= MIN_FREQUENCY].index\n",
    "\n",
    "with open(\"data/rare_ingredients.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(\"\\n\".join(sorted(rare_ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_ingredients(ingredients):\n",
    "    \"\"\"Remove rare ingredients from a list.\"\"\"\n",
    "    return [\n",
    "        ingredient\n",
    "        for ingredient in ingredients\n",
    "        if ingredient not in rare_ingredients\n",
    "    ]\n",
    "\n",
    "reduced_df.loc[:, \"cleaned_ingredients\"] = (\n",
    "    reduced_df[\"cleaned_ingredients\"].apply(remove_rare_ingredients)\n",
    ")\n",
    "\n",
    "plot_ingredient_analysis(reduced_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove recipes with very few ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INGREDIENT = 4\n",
    "MAX_INGREDIENT = 9\n",
    "\n",
    "counts = reduced_df[\"cleaned_ingredients\"].apply(len)\n",
    "reduced_df = reduced_df[(counts >= MIN_INGREDIENT) & (counts <= MAX_INGREDIENT)]\n",
    "\n",
    "plot_ingredient_analysis(reduced_df[\"cleaned_ingredients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "ingredients_as_texts = [\n",
    "    \" \".join(sorted(ingredients))\n",
    "    for ingredients in reduced_df[\"cleaned_ingredients\"]\n",
    "]\n",
    "embeddings = model.encode(\n",
    "    ingredients_as_texts,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "dimensions_to_keep = 3\n",
    "svd = TruncatedSVD(n_components=dimensions_to_keep)\n",
    "reduced_embeddings = svd.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 7\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "cluster_labels = clustering.fit_predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "final_df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": reduced_embeddings[:, 0],\n",
    "        \"y\": reduced_embeddings[:, 1],\n",
    "        \"z\": reduced_embeddings[:, 2],\n",
    "        \"cluster\": cluster_labels.astype(str),\n",
    "        \"recipe_name\": reduced_df[\"name\"],\n",
    "        \"cleaned_ingredients\": [\n",
    "            \"<br>\".join(ingredients)\n",
    "            for ingredients in reduced_df[\"cleaned_ingredients\"]\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    final_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    z=\"z\",\n",
    "    color=\"cluster\",\n",
    "    hover_name=\"recipe_name\",\n",
    "    hover_data=[\"cleaned_ingredients\"],\n",
    "    width=800,\n",
    "    height=1000,\n",
    "    color_discrete_sequence=px.colors.qualitative.Bold,\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title=\"Ingredient-Based Clustering of Filipino Dishes\")\n",
    "\n",
    "unique_clusters = final_df[\"cluster\"].unique()\n",
    "buttons = list(\n",
    "    [\n",
    "        dict(\n",
    "            label=f\"Cluster {cluster}\",\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"visible\": [cluster == c for c in unique_clusters]},\n",
    "                {\"title\": f\"Ingredient-Based Clustering - Cluster {cluster}\"},\n",
    "            ],\n",
    "        )\n",
    "        for cluster in unique_clusters\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            direction=\"down\",\n",
    "            buttons=buttons,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
